{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import process_data\n",
    "# from data_objects.utterance import Utterance\n",
    "import os, yaml, sys, random, torchaudio\n",
    "import pyworld as pw\n",
    "sys.path.insert(1, '/homes/bdoc3/my_utils')\n",
    "from audio.worldvocoder import code_harmonic, sp_to_mfsc\n",
    "from my_os import recursive_file_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, librosa, warnings\n",
    "\n",
    "# UTTERANCE CLASS\n",
    "\n",
    "class Utterance:\n",
    "    def __init__(self, frames_fpath, wave_fpath, config, feat_params):\n",
    "        self.frames_fpath = frames_fpath\n",
    "        self.wave_fpath = wave_fpath\n",
    "        self.config = config\n",
    "        self.feat_params = feat_params\n",
    "\n",
    "    def get_chunk(self, frames, n_frames, start=None):\n",
    "\n",
    "        if frames.shape[0] > n_frames:\n",
    "            if start == None:\n",
    "                start = np.random.randint(0, frames.shape[0] - n_frames)\n",
    "        else:\n",
    "#             print(f'frames.shape[0] {frames.shape[0]}, n_frames {n_frames}')\n",
    "            start = 0\n",
    "            pad_size = math.ceil(n_frames - frames.shape[0]/2)\n",
    "            if frames.ndim == 1:\n",
    "                pad_vec = np.full((pad_size), np.min(frames))\n",
    "            else:\n",
    "                pad_vec = np.full((pad_size, frames.shape[1]), np.min(frames))\n",
    "            frames = np.concatenate((pad_vec, frames, pad_vec))\n",
    "            \n",
    "        end = start + n_frames\n",
    "#         print('start', start)\n",
    "        return frames[start:end], (start, end)\n",
    "\n",
    "# get features, either from audio or precomputed npy arrays.\n",
    "    def get_frames(self, n_frames, start=None):\n",
    "\n",
    "        if self.config.use_audio:\n",
    "            y, _ = sf.read(self.frames_fpath)\n",
    "            samps_per_frame = (self.feat_params['frame_dur_ms']/1000) * self.feat_params['sr']\n",
    "            required_size =  int(samps_per_frame * n_frames)\n",
    "            if y.shape[0] < 1:\n",
    "                # '+2' at end is for f0_estimation vector\n",
    "                frames = np.zeros((n_frames, (self.feat_params['num_feats']+self.feat_params['num_aper_feats']+2)))\n",
    "                start_end = (0, required_size)\n",
    "            else:\n",
    "                counter = 0\n",
    "                looper = True\n",
    "                while looper:\n",
    "                    if counter > 10:\n",
    "                        raise Exception(f'Could not find vocal segments after randomly selecting 10 segments of length {n_frame}.')\n",
    "                    try:\n",
    "                        if start == None:\n",
    "                            y_chunk, start_end = self.get_chunk(y, required_size)\n",
    "                        else:\n",
    "                            y_chunk, start_end = self.get_chunk(y, required_size, start)\n",
    "                        frames = process_data(y_chunk.astype('double'), self.feat_params, self.config)\n",
    "                        looper = False\n",
    "                    except ValueError as e:\n",
    "                        print(e, 'Trying another random chunk')\n",
    "                        counter +=1\n",
    "\n",
    "        else:\n",
    "            frames = np.load(self.frames_fpath)\n",
    "            frames, start_end = self.get_chunk(frames, n_frames)\n",
    "        # print('another utterance processed')\n",
    "        return frames[:n_frames], start_end\n",
    "\n",
    "    def random_partial(self, n_frames, num_feats):\n",
    "        \"\"\"\n",
    "        Crops the frames into a partial utterance of n_frames\n",
    "        \n",
    "        :param n_frames: The number of frames of the partial utterance\n",
    "        :return: the partial utterance frames and a tuple indicating the start and end of the \n",
    "        partial utterance in the complete utterance.\n",
    "        \"\"\"\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        frames, start_end = self.get_frames(n_frames)\n",
    "        frames = frames[:,:num_feats]\n",
    "\n",
    "        # frames = (frames - frames.mean()) / frames.std() # normalise from 0-1 across entire numpy\n",
    "        # frames = (frames - frames.mean(axis=0)) / frames.std(axis=0) # normalise from 0-1 across features\n",
    "        # pdb.set_trace()   \n",
    "        return frames, start_end\n",
    "\n",
    "    def specific_partial(self, n_frames, num_feats, start):\n",
    "        \"\"\"\n",
    "        Crops the frames into a partial utterance of n_frames\n",
    "        \n",
    "        :param n_frames: The number of frames of the partial utterance\n",
    "        :return: the partial utterance frames and a tuple indicating the start and end of the \n",
    "        partial utterance in the complete utterance.\n",
    "        \"\"\"\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        frames, start_end = self.get_frames(n_frames, start)\n",
    "        frames = frames[:,:num_feats]\n",
    "\n",
    "        # frames = (frames - frames.mean()) / frames.std() # normalise from 0-1 across entire numpy\n",
    "        # frames = (frames - frames.mean(axis=0)) / frames.std(axis=0) # normalise from 0-1 across features\n",
    "        # pdb.set_trace()   \n",
    "        return frames, start_end \n",
    "\n",
    "def process_data(y, feat_params, config):\n",
    "\n",
    "    if config.use_wav2world:\n",
    "        feats=pw.wav2world(y, feat_params['sr'],frame_period=feat_params['frame_dur_ms'])\n",
    "        harm = feats[1]\n",
    "        aper = feats[2]\n",
    "        refined_f0 = feats[0]\n",
    "    else:\n",
    "        if config.f0_extract == 'harvest':\n",
    "            f0, t_stamp = pw.harvest(y, feat_params['sr'], feat_params['fmin'], feat_params['fmax'], feat_params['frame_dur_ms'])\n",
    "        elif config.f0_extract =='dio':\n",
    "            f0, t_stamp = pw.dio(y, feat_params['sr'], feat_params['fmin'], feat_params['fmax'], frame_period = feat_params['frame_dur_ms'])\n",
    "        refined_f0 = pw.stonemask(y, f0, t_stamp, feat_params['sr'])\n",
    "        harm = pw.cheaptrick(y, refined_f0, t_stamp, feat_params['sr'], f0_floor=feat_params['fmin'])\n",
    "        aper = pw.d4c(y, refined_f0, t_stamp, feat_params['sr'])\n",
    "    refined_f0 = freq_to_vuv_midi(refined_f0) # <<< this can be done at training time\n",
    "\n",
    "    # print('basic harm/aper/f0 features extracted')\n",
    "\n",
    "    if config.dim_red_method == 'code-h':\n",
    "        harm = code_harmonic(harm, feat_params['num_feats'])\n",
    "        aper = code_harmonic(aper, feat_params['num_aper_feats'])\n",
    "    elif config.dim_red_method == 'world':\n",
    "        harm = pw.code_spectral_envelope(harm, feat_params['sr'], feat_params['num_feats'])\n",
    "        aper = pw.code_aperiodicity(aper, feat_params['num_feats'])\n",
    "    elif config.dim_red_method == 'chandna':\n",
    "        harm = 10*np.log10(harm) # previously, using these logs was a separate optional process to 'chandna'\n",
    "        aper = 10*np.log10(aper**2)\n",
    "        harm = sp_to_mfsc(harm, feat_params['num_feats'], 0.45)\n",
    "        aper =sp_to_mfsc(aper, 4, 0.45)\n",
    "    else:\n",
    "        raise Exception(\"The value for dim_red_method was not recognised\")\n",
    "    # print(f'{random.randint(0,100)}feature dims reduced')\n",
    "\n",
    "\n",
    "    out_feats=np.concatenate((harm,aper,refined_f0),axis=1)\n",
    "\n",
    "    return out_feats\n",
    "\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isinf(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "\n",
    "def freq_to_vuv_midi(f0):\n",
    "    \"Convert to midi notes, with second vector displaying 1 when there's no pitch detected\"\n",
    "    with warnings.catch_warnings(): # warning \n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        notes_y = 69+12*np.log2(f0/440)\n",
    "    y = notes_y\n",
    "    \"Nan related\"\n",
    "    nans, x= nan_helper(y)\n",
    "    if np.all(nans) == True:\n",
    "        raise ValueError('No voice pitch detected in segment')\n",
    "    naners=np.isinf(y)\n",
    "    y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    y=np.array(y).reshape([len(y),1])\n",
    "    guy=np.array(naners).reshape([len(y),1])\n",
    "    y=np.concatenate((y,guy),axis=-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP'\n",
    "\n",
    "# with open(os.path.join(data_path, 'feat_params.yaml'), 'rb') as Handle:\n",
    "#     feat_params = yaml.load(Handle, Loader=yaml.FullLoader)\n",
    "\n",
    "class Object():\n",
    "    pass\n",
    "\n",
    "config = Object()\n",
    "config.use_audio = True\n",
    "config.use_wav2world = True\n",
    "config.f0_extract = 'harvest'\n",
    "config.dim_red_method = 'chandna'\n",
    "feat_params = {\"use_wav2world\":config.use_wav2world,\n",
    "                                \"f0_extract\":config.f0_extract,\n",
    "                                \"dim_red_method\":config.dim_red_method,\n",
    "                                \"fmin\":50,\n",
    "                                \"fmax\":1100,\n",
    "                                'num_feats':40,\n",
    "                                'num_aper_feats':4,\n",
    "                                'frame_dur_ms':10,\n",
    "                                'sr':16000,\n",
    "                                'fft_size':None}\n",
    "n_frames = 307\n",
    "num_feats = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1006811699_1843697812.wav, process 0\n",
      "start 1534031\n",
      "start 1965938\n",
      "start 846338\n",
      "start 1179270\n",
      "start 681702\n",
      "start 137574\n",
      "start 1343017\n",
      "start 96237\n",
      "start 1510890\n",
      "start 794940\n",
      "start 667331\n",
      "start 1889658\n",
      "start 285500\n",
      "start 1509357\n",
      "start 1409037\n",
      "start 2888039\n",
      "start 43960\n",
      "start 1068158\n",
      "start 863627\n",
      "start 27331\n",
      "start 693680\n",
      "start 2848947\n",
      "start 844222\n",
      "start 453365\n"
     ]
    }
   ],
   "source": [
    "# issue_singers = ['1468732648']\n",
    "issue_singers = ['1006811699']\n",
    "# issue_singers = ['424702701']\n",
    "\n",
    "uttrs = []\n",
    "counter = 0\n",
    "while counter<20:\n",
    "    for i in issue_singers:\n",
    "        dir_path = os.path.join(data_path, 'train', i)\n",
    "        if not os.path.exists(dir_path):\n",
    "            dir_path = os.path.join(data_path, 'val', i)\n",
    "        _, _, files = next(os.walk(dir_path))\n",
    "        for file_name in files:\n",
    "            print(f'Processing {file_name}, process {counter}')\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            u = Utterance(file_path, file_path, config, feat_params)\n",
    "            u_part = u.random_partial(n_frames, num_feats)\n",
    "            uttrs.append(u_part)\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with specific example\n",
    "\n",
    "ex = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP/train/424702701/424702701_1734334861.wav'\n",
    "start = 1425394\n",
    "u = Utterance(ex, ex, config, feat_params)\n",
    "u.random_partial(n_frames, num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "44.2 ms ± 23.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "all_dirs, files = recursive_file_retrieval(data_path)\n",
    "wav_files = [f for f in files if f.endswith('wav')]\n",
    "\n",
    "# %timeit librosa.load(wav_files[random.randint(0, len(wav_files))]) #  1.57 s ± 631 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# %timeit sf.read(wav_files[random.randint(0, len(wav_files))]) #53.8 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "# %timeit torchaudio.load(wav_files[random.randint(0, len(wav_files))]) #44.2 ms ± 23.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "y.shape[0] is 16000, s is 16000\n",
      "y.shape[0] is 16000, required_size is 49120\n",
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n",
      "947 ms ± 45.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# with librosa IO - 1.98 s ± 220 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "file_path = wav_files[random.randint(0, len(wav_files))]\n",
    "u = Utterance(file_path, file_path, config, feat_params)\n",
    "u_part = u.random_partial(n_frames, num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 664268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ -5.53413142,  -5.16761401,  -6.13489713, ..., -23.56622401,\n",
       "         -24.75902111, -35.46205331],\n",
       "        [ -7.44956002,  -8.1365492 ,  -7.68187535, ..., -22.74666417,\n",
       "         -23.07679609, -32.91781073],\n",
       "        [ -7.82786729,  -8.06915859,  -7.88852626, ..., -24.24993222,\n",
       "         -24.08777826, -32.59682484],\n",
       "        ...,\n",
       "        [ -4.24098048,  -5.18928258,  -4.57552981, ..., -21.62389486,\n",
       "         -20.34128273, -28.53943275],\n",
       "        [ -4.33712282,  -5.15864007,  -4.64674877, ..., -21.21071475,\n",
       "         -19.01734806, -30.59034519],\n",
       "        [ -4.28971528,  -4.9698928 ,  -4.67096784, ..., -19.60396634,\n",
       "         -22.53332584, -33.21100276]]),\n",
       " (664268, 713388))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = wav_files[random.randint(0, len(wav_files))]\n",
    "file_path = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP/train/374399338/374399338_1741657119.wav'\n",
    "u = Utterance(file_path, file_path, config, feat_params)\n",
    "u.frames_fpath\n",
    "u.random_partial(307, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames.shape[0] 16000, n_frames 49120\n",
      "start 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-74.45549213, -74.51635275, -75.16445553, ..., -73.59117998,\n",
       "         -73.65194068, -72.67943397],\n",
       "        [-73.9299961 , -73.77400554, -73.17402136, ..., -72.65817129,\n",
       "         -73.17156661, -73.65419276],\n",
       "        [-72.97933636, -73.16329091, -73.07653586, ..., -73.91498246,\n",
       "         -73.36513359, -73.14998229],\n",
       "        ...,\n",
       "        [-16.52725893, -16.92269527, -16.2995616 , ..., -22.22038767,\n",
       "         -25.00117668, -36.94109211],\n",
       "        [-17.82533619, -17.51704452, -19.22908632, ..., -22.72612031,\n",
       "         -23.27058227, -32.21532981],\n",
       "        [-17.97780601, -18.45214683, -17.85974323, ..., -24.0291684 ,\n",
       "         -27.3111865 , -34.40743662]]),\n",
       " (0, 49120))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwe, asd = sf.read('/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP/train/374399338/374399338_1741657119.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03210449, -0.05941772, -0.10882568, ...,  0.06515503,\n",
       "         0.06286621,  0.06729126]),\n",
       " 16000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe, asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732608"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f62c09e4756faa82e7be093ccf48d7a4ce580f4dbfadca0d80f9d90a0320dae2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
