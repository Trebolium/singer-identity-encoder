{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import process_data\n",
    "# from data_objects.utterance import Utterance\n",
    "import os, yaml, sys, random, torchaudio\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "\n",
    "for i in sys.path:\n",
    "    if i == '/homes/bdoc3/wavenet_vocoder':\n",
    "        sys.path.remove(i)\n",
    "\n",
    "sys.path.insert(1, '/homes/bdoc3/my_utils')\n",
    "from audio.worldvocoder import code_harmonic, sp_to_mfsc\n",
    "from my_os import recursive_file_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, librosa, warnings, pdb\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\"\"\"Minimally altered code from https://github.com/Trebolium/Real-Time-Voice-Cloning/tree/master/encoder/data_objects\"\"\"\n",
    "\n",
    "class Utterance:\n",
    "    def __init__(self, frames_fpath, wave_fpath, config, feat_params):\n",
    "        self.frames_fpath = frames_fpath\n",
    "        self.wave_fpath = wave_fpath\n",
    "        self.config = config\n",
    "        self.feat_params = feat_params\n",
    "        if config.feats_type == 'mel':\n",
    "            num_total_feats = feat_params['num_harm_feats']\n",
    "            self.mel_filter = mel(config.sampling_rate, config.fft_size, fmin=config.fmin, fmax=config.fmax, n_mels=num_total_feats).T\n",
    "            # self.mel_filter = mel(16000, 1024, fmin=90, fmax=7600, n_mels=80).T\n",
    "            self.min_level = np.exp(-100 / 20 * np.log(10))\n",
    "            self.hop_size = int((self.config.frame_dur_ms/1000) * self.config.sampling_rate)\n",
    "\n",
    "            \n",
    "    def get_chunk(self, frames, n_frames, start=None):\n",
    "\n",
    "        if frames.shape[0] > n_frames:\n",
    "            if start == None:\n",
    "                start = np.random.randint(0, frames.shape[0] - n_frames)\n",
    "        else:\n",
    "#             print(f'frames.shape[0] {frames.shape[0]}, n_frames {n_frames}')\n",
    "            start = 0\n",
    "            pad_size = math.ceil(n_frames - frames.shape[0]/2)\n",
    "            if frames.ndim == 1:\n",
    "                pad_vec = np.full((pad_size), np.min(frames))\n",
    "            else:\n",
    "                pad_vec = np.full((pad_size, frames.shape[1]), np.min(frames))\n",
    "            frames = np.concatenate((pad_vec, frames, pad_vec))\n",
    "            \n",
    "        end = start + n_frames\n",
    "        # print('start', start)\n",
    "        return frames[start:end], (start, end)\n",
    "\n",
    "    \n",
    "# get features, either from audio or precomputed npy arrays.\n",
    "    def get_frames(self, n_frames, start=None):\n",
    "\n",
    "        if self.config.use_audio:\n",
    "            _, y = wavfile.read(self.frames_fpath)\n",
    "            samps_per_frame = (self.feat_params['frame_dur_ms']/1000) * self.feat_params['sr']\n",
    "            required_size =  int(samps_per_frame * n_frames)\n",
    "            if y.shape[0] < 1:\n",
    "                # '+2' at end is for f0_estimation vectors\n",
    "                frames = np.zeros((n_frames, (self.feat_params['num_harm_feats']+self.feat_params['num_aper_feats']+2)))\n",
    "                start_end = (0, required_size)\n",
    "            else:\n",
    "                counter = 0\n",
    "                looper = True\n",
    "                while looper:\n",
    "                    if counter < 10:\n",
    "                        try:\n",
    "                            if start == None:\n",
    "                                y_chunk, start_end = self.get_chunk(y, required_size)\n",
    "                            else:\n",
    "                                y_chunk, start_end = self.get_chunk(y, required_size, start)\n",
    "                            if self.config.feats_type == 'mel':\n",
    "                                db_unnormed_melspec = audio_to_mel_autovc(y_chunk, self.config.fft_size, self.hop_size, self.mel_filter)\n",
    "                                frames = db_normalize(db_unnormed_melspec, self.min_level)\n",
    "                            elif self.config.feats_type == 'world':\n",
    "                                frames = get_world_feats(y_chunk.astype('double'), self.feat_params, self.config)\n",
    "                            \n",
    "                            looper = False\n",
    "                        except ValueError as e:\n",
    "                            print(f'ValueError: {e}. Trying another random chunk from uttr: {self.frames_fpath}')\n",
    "                            counter +=1\n",
    "                    else:\n",
    "                        print(f'Could not find vocal segments. Returning zero\\'d array instead')\n",
    "                        frames = np.zeros((n_frames, (self.feat_params['num_harm_feats']+self.feat_params['num_aper_feats']+2))) # might need to alter if making aper gens conditional of config.use_aper_feats\n",
    "                        start_end = (0, required_size)\n",
    "                        looper = False\n",
    "        else:\n",
    "            frames = np.load(self.frames_fpath)\n",
    "            frames, start_end = self.get_chunk(frames, n_frames)\n",
    "        # print('another utterance processed')\n",
    "        return frames[:n_frames], start_end\n",
    "\n",
    "    \n",
    "    def random_partial(self, n_frames, num_total_feats):\n",
    "        \"\"\"\n",
    "        Crops the frames into a partial utterance of n_frames\n",
    "        \n",
    "        :param n_frames: The number of frames of the partial utterance\n",
    "        :return: the partial utterance frames and a tuple indicating the start and end of the \n",
    "        partial utterance in the complete utterance.\n",
    "        \"\"\"\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        frames, start_end = self.get_frames(n_frames)\n",
    "        frames = frames[:,:num_total_feats]\n",
    "\n",
    "        # frames = (frames - frames.mean()) / frames.std() # normalise from 0-1 across entire numpy\n",
    "        # frames = (frames - frames.mean(axis=0)) / frames.std(axis=0) # normalise from 0-1 across features\n",
    "        # pdb.set_trace()   \n",
    "        return frames, start_end\n",
    "\n",
    "    \n",
    "    def specific_partial(self, n_frames, num_total_feats, start):\n",
    "        \"\"\"\n",
    "        Crops the frames into a partial utterance of n_frames\n",
    "        \n",
    "        :param n_frames: The number of frames of the partial utterance\n",
    "        :return: the partial utterance frames and a tuple indicating the start and end of the \n",
    "        partial utterance in the complete utterance.\n",
    "        \"\"\"\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        frames, start_end = self.get_frames(n_frames, start)\n",
    "        frames = frames[:,:num_total_feats]\n",
    "\n",
    "        # frames = (frames - frames.mean()) / frames.std() # normalise from 0-1 across entire numpy\n",
    "        # frames = (frames - frames.mean(axis=0)) / frames.std(axis=0) # normalise from 0-1 across features\n",
    "        # pdb.set_trace()   \n",
    "        return frames, start_end \n",
    "\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isinf(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "\n",
    "#Convert to midi notes, with second vector displaying 1 when there's no pitch detected\n",
    "def freq_to_vuv_midi(f0):\n",
    "    with warnings.catch_warnings(): # warning \n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        notes_y = 69+12*np.log2(f0/440)\n",
    "    y = notes_y\n",
    "    \"Nan related\"\n",
    "    nans, x= nan_helper(y)\n",
    "    if np.all(nans) == True:\n",
    "        raise ValueError('No voice pitch detected in segment')\n",
    "    naners=np.isinf(y)\n",
    "    y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    y=np.array(y).reshape([len(y),1])\n",
    "    guy=np.array(naners).reshape([len(y),1])\n",
    "    y=np.concatenate((y,guy),axis=-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_path, 'feat_params.yaml'), 'rb') as Handle:\n",
    "#     feat_params = yaml.load(Handle, Loader=yaml.FullLoader)\n",
    "\n",
    "class Object():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1468732648_1822074274.wav, process 0\n",
      "start 1934796\n",
      "Processing 1468732648_1822074274.wav, process 1\n",
      "start 1735895\n",
      "Processing 1468732648_1822074274.wav, process 2\n",
      "start 598058\n"
     ]
    }
   ],
   "source": [
    "config = Object()\n",
    "config.use_audio = True\n",
    "config.use_wav2world = True\n",
    "config.f0_extract = 'harvest'\n",
    "config.dim_red_method = 'chandna'\n",
    "feat_params = {\"use_wav2world\":config.use_wav2world,\n",
    "                                \"f0_extract\":config.f0_extract,\n",
    "                                \"dim_red_method\":config.dim_red_method,\n",
    "                                \"fmin\":71,\n",
    "                                \"fmax\":800,\n",
    "                                'num_feats':40,\n",
    "                                'num_aper_feats':4,\n",
    "                                'frame_dur_ms':5,\n",
    "                                'sr':16000,\n",
    "                                'fft_size':None}\n",
    "n_frames = 307\n",
    "num_feats = 40\n",
    "\n",
    "data_path = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP'\n",
    "\n",
    "s_ids = ['1468732648'] #\n",
    "# s_ids = ['1006811699'] # \n",
    "# s_ids = ['424702701'] # there is no audio data saved for this singer\n",
    "# s_ids = ['1418829056']\n",
    "\n",
    "uttrs = []\n",
    "counter = 0\n",
    "while counter<3:\n",
    "    for s_id in s_ids:\n",
    "        dir_path = os.path.join(data_path, 'train', s_id)\n",
    "        if not os.path.exists(dir_path):\n",
    "            dir_path = os.path.join(data_path, 'val', s_id)\n",
    "            if not os.path.exists(dir_path):\n",
    "                raise IOError('path doesn\\'t exist')\n",
    "        _, _, files = next(os.walk(dir_path))\n",
    "        for file_name in files:\n",
    "            print(f'Processing {file_name}, process {counter}')\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "            u = Utterance(file_path, file_path, config, feat_params)\n",
    "            u_part = u.random_partial(n_frames, num_feats)\n",
    "#             u_part = u.specific_partial(n_frames, num_feats, 31000)\n",
    "            uttrs.append(u_part)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use original dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks/1418829056_1704865349.m4a, process 0\n",
      "start 2795860\n",
      "Processing /import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks/1418829056_1704865349.m4a, process 1\n",
      "start 4688143\n",
      "Processing /import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks/1418829056_1704865349.m4a, process 2\n",
      "start 5744079\n"
     ]
    }
   ],
   "source": [
    "data_path = '/import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks'\n",
    "_, all_fps = recursive_file_retrieval(data_path)\n",
    "\n",
    "# issue_singers = ['1468732648'] #\n",
    "# issue_singers = ['1006811699'] # \n",
    "# issue_singers = ['424702701'] # there is no audio data saved for this singer\n",
    "s_id = '1418829056'\n",
    "\n",
    "\n",
    "\n",
    "uttrs = []\n",
    "counter = 0\n",
    "for fp in all_fps:\n",
    "    if s_id in fp:\n",
    "        while counter <3:\n",
    "            print(f'Processing {fp}, process {counter}')\n",
    "            u = Utterance(fp, fp, config, feat_params)\n",
    "            u_part = u.random_partial(n_frames, num_feats)\n",
    "    #             u_part = u.specific_partial(n_frames, num_feats, 31000)\n",
    "            uttrs.append(u_part)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP'\n",
    "\n",
    "all_dirs, files = recursive_file_retrieval(data_path)\n",
    "wav_files = [f for f in files if f.endswith('wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/import/c4dm-datasets/DAMP_Intonation_Dataset/vocal_tracks'\n",
    "\n",
    "all_dirs, files = recursive_file_retrieval(data_path)\n",
    "m4a_files = [f for f in files if f.endswith('m4a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit results for loading files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit librosa.load(wav_files[random.randint(0, len(wav_files))]) #  1.57 s ± 631 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# %timeit sf.read(wav_files[random.randint(0, len(wav_files))]) #53.8 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "# %timeit torchaudio.load(wav_files[random.randint(0, len(wav_files))]) #44.2 ms ± 23.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit results for loading and processing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 1951484\n",
      "start 1072990\n",
      "start 635304\n",
      "start 232686\n",
      "start 2397059\n",
      "start 1415726\n",
      "start 2148817\n",
      "start 910972\n",
      "235 ms ± 20.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# FOR HARM FEATS ONLY\n",
    "# with librosa IO on wav - 1.98 s ± 220 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# with sf IO on wav - 264 ms ± 14.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# with sf IO on m4a - 367 ms ± 53.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# with scipy.io.wavfile (on wav, obviously) - 221 ms ± 97.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "num_feats = 44\n",
    "# FOR HARM AND APER FEATS\n",
    "# WITH  scipy.io.wavfile (on wav, obviously) - \n",
    "\n",
    "file_path = wav_files[random.randint(0, len(wav_files))]\n",
    "u = Utterance(file_path, file_path, config, feat_params)\n",
    "u_part = u.random_partial(n_frames, num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 664268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ -5.53413142,  -5.16761401,  -6.13489713, ..., -23.56622401,\n",
       "         -24.75902111, -35.46205331],\n",
       "        [ -7.44956002,  -8.1365492 ,  -7.68187535, ..., -22.74666417,\n",
       "         -23.07679609, -32.91781073],\n",
       "        [ -7.82786729,  -8.06915859,  -7.88852626, ..., -24.24993222,\n",
       "         -24.08777826, -32.59682484],\n",
       "        ...,\n",
       "        [ -4.24098048,  -5.18928258,  -4.57552981, ..., -21.62389486,\n",
       "         -20.34128273, -28.53943275],\n",
       "        [ -4.33712282,  -5.15864007,  -4.64674877, ..., -21.21071475,\n",
       "         -19.01734806, -30.59034519],\n",
       "        [ -4.28971528,  -4.9698928 ,  -4.67096784, ..., -19.60396634,\n",
       "         -22.53332584, -33.21100276]]),\n",
       " (664268, 713388))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = wav_files[random.randint(0, len(wav_files))]\n",
    "file_path = '/homes/bdoc3/my_data/audio_data/deslienced_concat_DAMP/train/374399338/374399338_1741657119.wav'\n",
    "u = Utterance(file_path, file_path, config, feat_params)\n",
    "u.frames_fpath\n",
    "u.random_partial(307, 40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f62c09e4756faa82e7be093ccf48d7a4ce580f4dbfadca0d80f9d90a0320dae2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
